name: Generic LoRA Fine-tuning
description: Fine-tunes any LLM using PEFT LoRA adapters. Supports Hugging Face models with automatic target module detection. Produces LoRA adapters ready for merging or inference.

inputs:
  - {name: model_name, type: String, description: 'Hugging Face model ID or path (e.g., meta-llama/Llama-2-7b-hf)'}
  - {name: processed_dataset, type: Dataset, description: 'Prepared dataset from Generic Dataset Preparation component'}
  - {name: schema_json, type: Data, description: 'Schema metadata JSON from Generic Dataset Preparation component'}
  - {name: epochs, type: Integer, description: 'Number of training epochs'}
  - {name: batch_size, type: Integer, description: 'Per-device training batch size'}
  - {name: learning_rate, type: Float, description: 'Learning rate for optimizer'}
  - {name: grad_accum_steps, type: Integer, description: 'Gradient accumulation steps'}
  - {name: precision, type: String, description: 'Training precision: fp32, fp16, or bf16'}
  - {name: quantization_bits, type: Integer, description: 'Quantization bits (0=full, 4=4bit, 8=8bit)'}
  - {name: lora_r, type: Integer, description: 'LoRA rank parameter'}
  - {name: lora_alpha, type: Integer, description: 'LoRA alpha parameter'}
  - {name: lora_dropout, type: Float, description: 'LoRA dropout rate'}
  - {name: target_modules, type: String, description: 'Comma-separated LoRA target modules (auto-detect if empty)'}
  - {name: hf_token, type: String, description: 'Hugging Face access token for gated models'}

outputs:
  - {name: lora_adapters, type: Model}

implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import os
        import json
        import math
        import argparse
        import tempfile
        import shutil
        import torch
        from datasets import load_from_disk
        from transformers import (
            AutoTokenizer,
            AutoModelForCausalLM,
            DataCollatorForLanguageModeling,
            Trainer,
            TrainingArguments,
        )
        from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
        import logging
        
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger('lora_training')
        
        def ensure_directory_exists(file_path):
            directory = os.path.dirname(file_path)
            if directory and not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)
        
        def get_token_value(token_arg):
            if not token_arg:
                return None
            token_str = str(token_arg).strip()
            if os.path.exists(token_str):
                try:
                    with open(token_str, 'r') as f:
                        content = f.read().strip()
                        return content if content else None
                except Exception as e:
                    logger.error(f"Failed to read token file: {e}")
                    return None
            else:
                return token_str if token_str and token_str != "None" else None
        
        def load_schema(ds_dir):
            schema_path = os.path.join(ds_dir, "schema.json")
            if not os.path.exists(schema_path):
                logger.warning(f"schema.json not found in {ds_dir}, using defaults")
                return {}
            with open(schema_path) as f:
                return json.load(f)
        
        def maybe_chat_template(tokenizer):
            tmpl = getattr(tokenizer, "chat_template", None)
            return tmpl is not None and len(str(tmpl)) > 0
        
        def guess_lora_targets(model):
            """Auto-detect LoRA target modules based on model architecture."""
            names = [n for n, _ in model.named_modules()]
            logger.info(f"Analyzing model architecture for LoRA targets...")
            
            if any("q_proj" in n for n in names):
                targets = ["q_proj", "v_proj"]
            elif any("c_attn" in n for n in names):
                targets = ["c_attn", "c_proj"]
            elif any("fc1" in n for n in names):
                targets = ["fc1", "fc2"]
            elif any("Wqkv" in n for n in names):
                targets = ["Wqkv"]
            elif any("query_key_value" in n for n in names):
                targets = ["query_key_value"]
            else:
                logger.warning("Could not auto-detect LoRA targets; defaulting to common linear layers")
                targets = ["q_proj", "v_proj"]
            
            logger.info(f"Selected LoRA target modules: {targets}")
            return targets
        
        parser = argparse.ArgumentParser(description="Generic LoRA fine-tuning")
        parser.add_argument("--model_name", required=True)
        parser.add_argument("--processed_dataset", required=True)
        parser.add_argument("--schema_json", required=True)
        parser.add_argument("--epochs", type=int, default=1)
        parser.add_argument("--batch_size", type=int, default=2)
        parser.add_argument("--learning_rate", type=float, default=2e-4)
        parser.add_argument("--grad_accum_steps", type=int, default=4)
        parser.add_argument("--precision", default="fp32")
        parser.add_argument("--quantization_bits", type=int, default=0)
        parser.add_argument("--lora_r", type=int, default=8)
        parser.add_argument("--lora_alpha", type=int, default=16)
        parser.add_argument("--lora_dropout", type=float, default=0.05)
        parser.add_argument("--target_modules", default="")
        parser.add_argument("--hf_token", default="")
        parser.add_argument("--lora_adapters", required=True)
        args = parser.parse_args()
        
        hf_token = get_token_value(args.hf_token)
        logger.info(f"HF Token present: {bool(hf_token)}")
        
        ensure_directory_exists(args.lora_adapters)
        output_dir = args.lora_adapters
        
        logger.info(f"Loading schema from {args.schema_json}")
        with open(args.schema_json, 'r') as f:
            schema = json.load(f)
        logger.info(f"Schema loaded: {json.dumps(schema, indent=2)}")
        
        logger.info(f"Loading dataset from {args.processed_dataset}")
        dsd = load_from_disk(args.processed_dataset)
        
        logger.info(f"Loading model/tokenizer from: {args.model_name}")
        
        tokenizer_kwargs = {
            "use_fast": True,
            "trust_remote_code": True
        }
        if hf_token:
            tokenizer_kwargs["token"] = hf_token
        
        tokenizer = AutoTokenizer.from_pretrained(args.model_name, **tokenizer_kwargs)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            logger.info("Set pad_token to eos_token")
        
        model_kwargs = {
            "trust_remote_code": True,
            "torch_dtype": torch.float32
        }
        if hf_token:
            model_kwargs["token"] = hf_token
        
        if args.precision == "bf16":
            model_kwargs["torch_dtype"] = torch.bfloat16
        elif args.precision == "fp16":
            model_kwargs["torch_dtype"] = torch.float16
        
        try:
            model_kwargs["use_safetensors"] = True
            model = AutoModelForCausalLM.from_pretrained(args.model_name, **model_kwargs)
        except Exception as e:
            logger.warning(f"Failed to load with safetensors, trying without: {e}")
            model_kwargs.pop("use_safetensors", None)
            model = AutoModelForCausalLM.from_pretrained(args.model_name, **model_kwargs)
        
        logger.info(f"Model loaded successfully: {type(model).__name__}")
        
        if args.quantization_bits in (4, 8):
            logger.info(f"Preparing model for {args.quantization_bits}-bit training")
            model = prepare_model_for_kbit_training(model)
        
        if args.target_modules.strip():
            target_modules = [t.strip() for t in args.target_modules.split(",") if t.strip()]
            logger.info(f"Using user-specified LoRA targets: {target_modules}")
        else:
            target_modules = guess_lora_targets(model)
        
        lora_cfg = LoraConfig(
            r=args.lora_r,
            lora_alpha=args.lora_alpha,
            lora_dropout=args.lora_dropout,
            target_modules=target_modules,
            bias="none",
            task_type="CAUSAL_LM"
        )
        
        model = get_peft_model(model, lora_cfg)
        model.print_trainable_parameters()
        
        has_messages = "messages" in dsd["train"].column_names
        use_chat = has_messages and maybe_chat_template(tokenizer)
        logger.info(f"Dataset has messages format: {has_messages}")
        logger.info(f"Using chat template: {use_chat}")
        
        def tokenize_example(ex):
            if has_messages:
                if use_chat:
                    try:
                        ids = tokenizer.apply_chat_template(
                            ex["messages"],
                            tokenize=True,
                            add_generation_prompt=False,
                            truncation=True,
                            max_length=2048
                        )
                        return {"input_ids": ids, "attention_mask": [1] * len(ids)}
                    except Exception as e:
                        logger.warning(f"Chat template failed, using fallback: {e}")
                        q = ex["messages"][0]["content"] if len(ex["messages"]) > 0 else ""
                        a = ex["messages"][1]["content"] if len(ex["messages"]) > 1 else ""
                        text = f"User: {q}\nAssistant: {a}"
                else:
                    q = ex["messages"][0]["content"] if len(ex["messages"]) > 0 else ""
                    a = ex["messages"][1]["content"] if len(ex["messages"]) > 1 else ""
                    text = f"User: {q}\nAssistant: {a}"
                
                if not use_chat:
                    out = tokenizer(text, truncation=True, max_length=2048)
                    return {"input_ids": out["input_ids"], "attention_mask": out["attention_mask"]}
            else:
                input_field = schema.get("input_field", "text")
                text = ex.get(input_field, "")
                out = tokenizer(text, truncation=True, max_length=2048)
                return {"input_ids": out["input_ids"], "attention_mask": out["attention_mask"]}
        
        keep_cols = ["input_ids", "attention_mask"]
        logger.info("Tokenizing dataset...")
        dsd = dsd.map(
            tokenize_example,
            remove_columns=[c for c in dsd["train"].column_names if c not in keep_cols],
            desc="Tokenizing"
        )
        
        collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
        total_steps = max(1, math.ceil(len(dsd["train"]) / (args.batch_size * args.grad_accum_steps)))
        
        training_args = TrainingArguments(
            output_dir=output_dir,
            per_device_train_batch_size=args.batch_size,
            gradient_accumulation_steps=args.grad_accum_steps,
            learning_rate=args.learning_rate,
            num_train_epochs=args.epochs,
            warmup_ratio=0.03,
            logging_steps=max(1, total_steps // 10),
            save_steps=total_steps,
            save_total_limit=1,
            report_to="none",
            bf16=(args.precision == "bf16"),
            fp16=(args.precision == "fp16"),
            save_safetensors=True,
            dataloader_num_workers=0,
            remove_unused_columns=False,
        )
        
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=dsd["train"],
            data_collator=collator
        )
        
        logger.info("=" * 60)
        logger.info("Starting LoRA fine-tuning...")
        logger.info(f"Training samples: {len(dsd['train'])}")
        logger.info(f"Total steps: {total_steps}")
        logger.info(f"Effective batch size: {args.batch_size * args.grad_accum_steps}")
        logger.info("=" * 60)
        
        trainer.train()
        
        adapter_dir = os.path.join(output_dir, "adapter")
        os.makedirs(adapter_dir, exist_ok=True)
        
        logger.info(f"Saving LoRA adapters to {adapter_dir}")
        model.save_pretrained(adapter_dir)
        tokenizer.save_pretrained(output_dir)
        
        run_config = {
            "model_name": args.model_name,
            "epochs": args.epochs,
            "batch_size": args.batch_size,
            "learning_rate": args.learning_rate,
            "grad_accum_steps": args.grad_accum_steps,
            "precision": args.precision,
            "quantization_bits": args.quantization_bits,
            "lora_r": args.lora_r,
            "lora_alpha": args.lora_alpha,
            "lora_dropout": args.lora_dropout,
            "target_modules": target_modules,
            "total_steps": total_steps,
            "num_train_samples": len(dsd["train"])
        }
        
        with open(os.path.join(output_dir, "run_config.json"), "w") as f:
            json.dump(run_config, f, indent=2)
        
        logger.info("=" * 60)
        logger.info("✅ Fine-tuning complete!")
        logger.info(f"→ LoRA adapters saved to: {adapter_dir}")
        logger.info(f"→ Tokenizer saved to: {output_dir}")
        logger.info(f"→ Configuration saved to: {os.path.join(output_dir, 'run_config.json')}")
        logger.info("=" * 60)
        
    args:
      - --model_name
      - {inputValue: model_name}
      - --processed_dataset
      - {inputPath: processed_dataset}
      - --schema_json
      - {inputPath: schema_json}
      - --epochs
      - {inputValue: epochs}
      - --batch_size
      - {inputValue: batch_size}
      - --learning_rate
      - {inputValue: learning_rate}
      - --grad_accum_steps
      - {inputValue: grad_accum_steps}
      - --precision
      - {inputValue: precision}
      - --quantization_bits
      - {inputValue: quantization_bits}
      - --lora_r
      - {inputValue: lora_r}
      - --lora_alpha
      - {inputValue: lora_alpha}
      - --lora_dropout
      - {inputValue: lora_dropout}
      - --target_modules
      - {inputValue: target_modules}
      - --hf_token
      - {inputValue: hf_token}
      - --lora_adapters
      - {outputPath: lora_adapters}