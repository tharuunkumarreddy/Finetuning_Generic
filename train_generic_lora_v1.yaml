name: Generic LoRA Fine-Tuning
description: >
  Generic LoRA fine-tuning component using PEFT for Hugging Face models.
  Consumes processed dataset and schema.json from the dataset preparation step,
  trains a LoRA adapter, and saves the fine-tuned adapter and tokenizer artifacts.

inputs:
  - {name: model_name, type: String, description: 'Base model name or path from Hugging Face'}
  - {name: processed_dataset, type: Dataset, description: 'Path to prepared dataset from Generic Dataset Preparation'}
  - {name: schema_json, type: Data, description: 'Schema metadata JSON from dataset preparation'}
  - {name: epochs, type: Integer, default: 1, description: 'Number of fine-tuning epochs'}
  - {name: batch_size, type: Integer, default: 2, description: 'Batch size per device'}
  - {name: lr, type: Float, default: 0.0002, description: 'Learning rate'}
  - {name: grad_accum, type: Integer, default: 4, description: 'Gradient accumulation steps'}
  - {name: precision, type: String, default: 'fp32', description: 'Precision mode: fp32, fp16, or bf16'}
  - {name: bits, type: Integer, default: 0, description: 'Quantization bits (0, 4, or 8)'}
  - {name: lora_r, type: Integer, default: 8, description: 'LoRA rank dimension'}
  - {name: lora_alpha, type: Integer, default: 16, description: 'LoRA alpha scaling factor'}
  - {name: lora_dropout, type: Float, default: 0.05, description: 'LoRA dropout probability'}
  - {name: target_modules, type: String, default: '', description: 'Comma-separated target module names (optional)'}

outputs:
  - {name: lora_out, type: Model, description: 'Output directory containing LoRA adapters, tokenizer, and config files'}

implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import os
        import json
        import math
        import torch
        import argparse
        from datasets import load_from_disk
        from transformers import (
            AutoTokenizer,
            AutoModelForCausalLM,
            DataCollatorForLanguageModeling,
            Trainer,
            TrainingArguments,
        )
        from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

        # ------------------ Argparse ------------------
        parser = argparse.ArgumentParser(description="Generic LoRA fine-tuning")
        parser.add_argument("--model_name", required=True)
        parser.add_argument("--processed_dataset", required=True)
        parser.add_argument("--schema_json", required=True)
        parser.add_argument("--epochs", type=int, default=1)
        parser.add_argument("--batch_size", type=int, default=2)
        parser.add_argument("--lr", type=float, default=2e-4)
        parser.add_argument("--grad_accum", type=int, default=4)
        parser.add_argument("--precision", default="fp32", choices=["fp32", "fp16", "bf16"])
        parser.add_argument("--bits", type=int, default=0, choices=[0, 4, 8])
        parser.add_argument("--lora_r", type=int, default=8)
        parser.add_argument("--lora_alpha", type=int, default=16)
        parser.add_argument("--lora_dropout", type=float, default=0.05)
        parser.add_argument("--target_modules", default="")
        parser.add_argument("--lora_out", required=True)
        args = parser.parse_args()

        os.makedirs(args.lora_out, exist_ok=True)

        print(f"[INFO] Loading dataset from {args.processed_dataset}")
        dsd = load_from_disk(args.processed_dataset)

        print(f"[INFO] Loading schema from {args.schema_json}")
        with open(args.schema_json) as f:
            schema = json.load(f)

        # ------------------ Model & Tokenizer ------------------
        print(f"[INFO] Loading model/tokenizer from Hugging Face: {args.model_name}")
        tokenizer = AutoTokenizer.from_pretrained(args.model_name, use_fast=True, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        model = AutoModelForCausalLM.from_pretrained(
            args.model_name,
            use_safetensors=True,
            trust_remote_code=True
        )

        if args.bits in (4, 8):
            model = prepare_model_for_kbit_training(model)

        # ------------------ Auto-detect LoRA targets ------------------
        def guess_lora_targets(model):
            names = [n for n, _ in model.named_modules()]
            if any("q_proj" in n for n in names):
                return ["q_proj", "v_proj"]
            elif any("c_attn" in n for n in names):
                return ["c_attn", "c_proj"]
            elif any("fc1" in n for n in names):
                return ["fc1", "fc2"]
            elif any("Wqkv" in n for n in names):
                return ["Wqkv"]
            elif any("query_key_value" in n for n in names):
                return ["query_key_value"]
            print("[WARN] Could not auto-detect LoRA targets; defaulting to all Linear layers.")
            return ["lm_head"]

        if args.target_modules.strip():
            target_modules = [t.strip() for t in args.target_modules.split(",") if t.strip()]
        else:
            target_modules = guess_lora_targets(model)
            print(f"[INFO] Auto-detected LoRA target modules: {target_modules}")

        # ------------------ LoRA Config ------------------
        lora_cfg = LoraConfig(
            r=args.lora_r,
            lora_alpha=args.lora_alpha,
            lora_dropout=args.lora_dropout,
            target_modules=target_modules,
            bias="none",
            task_type="CAUSAL_LM"
        )
        model = get_peft_model(model, lora_cfg)
        model.print_trainable_parameters()

        # ------------------ Tokenization ------------------
        has_messages = schema.get("has_messages", False)
        def maybe_chat_template(tokenizer):
            tmpl = getattr(tokenizer, "chat_template", None)
            return tmpl is not None and len(str(tmpl)) > 0

        use_chat = has_messages and maybe_chat_template(tokenizer)
        print(f"[INFO] Chat template supported: {use_chat}")

        def tokenize_example(ex):
            if has_messages:
                if use_chat:
                    ids = tokenizer.apply_chat_template(
                        ex["messages"],
                        tokenize=True,
                        add_generation_prompt=True,
                        truncation=True
                    )
                    return {"input_ids": ids, "attention_mask": [1] * len(ids)}
                else:
                    q = ex["messages"][0]["content"] if len(ex["messages"]) > 0 else ""
                    a = ex["messages"][1]["content"] if len(ex["messages"]) > 1 else ""
                    text = f"User: {q}\nAssistant: {a}"
                    out = tokenizer(text, truncation=True)
                    return {"input_ids": out["input_ids"], "attention_mask": out["attention_mask"]}
            else:
                text = ex.get(schema.get("input_field"), "")
                out = tokenizer(text, truncation=True)
                return {"input_ids": out["input_ids"], "attention_mask": out["attention_mask"]}

        keep_cols = ["input_ids", "attention_mask"]
        dsd = dsd.map(tokenize_example, remove_columns=[c for c in dsd["train"].column_names if c not in keep_cols])

        collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
        total_steps = max(1, math.ceil(len(dsd["train"]) / (args.batch_size * args.grad_accum)))

        # ------------------ Training ------------------
        from transformers import TrainingArguments, Trainer
        training_args = TrainingArguments(
            output_dir=args.lora_out,
            per_device_train_batch_size=args.batch_size,
            gradient_accumulation_steps=args.grad_accum,
            learning_rate=args.lr,
            num_train_epochs=args.epochs,
            warmup_ratio=0.03,
            logging_steps=max(1, total_steps // 10),
            save_steps=total_steps,
            save_total_limit=1,
            report_to="none",
            bf16=(args.precision == "bf16"),
            fp16=(args.precision == "fp16"),
        )

        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=dsd["train"],
            data_collator=collator
        )

        print("[INFO] Starting LoRA fine-tuning...")
        trainer.train()

        # ------------------ Save Outputs ------------------
        adapter_dir = os.path.join(args.lora_out, "adapter")
        model.save_pretrained(adapter_dir)
        tokenizer.save_pretrained(args.lora_out)

        with open(os.path.join(args.lora_out, "run_config.json"), "w") as f:
            json.dump(vars(args), f, indent=2)

        print("\\n✅ Fine-tuning complete!")
        print(f"→ LoRA adapters saved to: {adapter_dir}")
        print(f"→ You can now merge them with the base model.\\n")
    args:
      - --model_name
      - {inputValue: model_name}
      - --processed_dataset
      - {inputPath: processed_dataset}
      - --schema_json
      - {inputPath: schema_json}
      - --epochs
      - {inputValue: epochs}
      - --batch_size
      - {inputValue: batch_size}
      - --lr
      - {inputValue: lr}
      - --grad_accum
      - {inputValue: grad_accum}
      - --precision
      - {inputValue: precision}
      - --bits
      - {inputValue: bits}
      - --lora_r
      - {inputValue: lora_r}
      - --lora_alpha
      - {inputValue: lora_alpha}
      - --lora_dropout
      - {inputValue: lora_dropout}
      - --target_modules
      - {inputValue: target_modules}
      - --lora_out
      - {outputPath: lora_out}
