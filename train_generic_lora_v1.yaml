name: Generic LoRA Fine-tuning
description: >
  Fine-tunes any LLM using PEFT LoRA adapters. Supports Hugging Face models with
  automatic target module detection. Produces LoRA adapters ready for merging or inference.

inputs:
  - {name: model_name, type: String, description: 'Hugging Face model ID or path (e.g., meta-llama/Llama-2-7b-hf)'}
  - {name: processed_dataset, type: Dataset, description: 'Prepared dataset from Generic Dataset Preparation component'}
  - {name: schema_json, type: Data, description: 'Schema metadata JSON from Generic Dataset Preparation component'}
  - {name: epochs, type: Integer, description: 'Number of training epochs', default: '1'}
  - {name: batch_size, type: Integer, description: 'Per-device training batch size', default: '2'}
  - {name: learning_rate, type: Float, description: 'Learning rate for optimizer', default: '0.0002'}
  - {name: grad_accum_steps, type: Integer, description: 'Gradient accumulation steps', default: '4'}
  - {name: precision, type: String, description: 'Training precision: fp32, fp16, or bf16', default: 'fp32'}
  - {name: quantization_bits, type: Integer, description: 'Quantization bits (0=full, 4=4bit, 8=8bit)', default: '0'}
  - {name: lora_r, type: Integer, description: 'LoRA rank parameter', default: '8'}
  - {name: lora_alpha, type: Integer, description: 'LoRA alpha parameter', default: '16'}
  - {name: lora_dropout, type: Float, description: 'LoRA dropout rate', default: '0.05'}
  - {name: target_modules, type: String, description: 'Comma-separated LoRA target modules (auto-detect if empty)', default: ''}
  - {name: hf_token, type: String, description: 'Hugging Face access token for gated models', default: ''}

outputs:
  - {name: lora_adapters, type: Model}

implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        set -e
        echo "Installing dependencies..."
        pip install --no-cache-dir peft > /dev/null 2>&1
        echo "Dependencies installed. Starting training script..."
        
        # Write Python script to temporary file
        cat > /tmp/train_lora.py << 'EOF'
        import os
        import sys
        import json
        import math
        import argparse
        import tempfile
        import shutil
        import torch
        from datasets import load_from_disk
        from transformers import (
            AutoTokenizer,
            AutoModelForCausalLM,
            DataCollatorForLanguageModeling,
            Trainer,
            TrainingArguments,
        )
        from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
        import logging

        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger('lora_training')

        def ensure_directory_exists(file_path):
            directory = os.path.dirname(file_path)
            if directory and not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)

        def get_token_value(token_arg):
            if not token_arg:
                return None
            token_str = str(token_arg).strip()
            if os.path.exists(token_str):
                try:
                    with open(token_str, 'r') as f:
                        content = f.read().strip()
                        return content if content else None
                except Exception as e:
                    logger.error(f"Failed to read token file: {e}")
                    return None
            else:
                return token_str if token_str and token_str != "None" else None

        def maybe_chat_template(tokenizer):
            tmpl = getattr(tokenizer, "chat_template", None)
            return tmpl is not None and len(str(tmpl)) > 0

        def guess_lora_targets(model):
            names = [n for n, _ in model.named_modules()]
            logger.info("Analyzing model architecture for LoRA targets...")
            if any("q_proj" in n for n in names):
                targets = ["q_proj", "v_proj"]
            elif any("c_attn" in n for n in names):
                targets = ["c_attn", "c_proj"]
            elif any("fc1" in n for n in names):
                targets = ["fc1", "fc2"]
            elif any("Wqkv" in n for n in names):
                targets = ["Wqkv"]
            elif any("query_key_value" in n for n in names):
                targets = ["query_key_value"]
            else:
                logger.warning("Could not auto-detect LoRA targets; defaulting to common linear layers")
                targets = ["q_proj", "v_proj"]
            logger.info(f"Selected LoRA target modules: {targets}")
            return targets

        parser = argparse.ArgumentParser(description="Generic LoRA fine-tuning")
        parser.add_argument("--model_name", required=True)
        parser.add_argument("--processed_dataset", required=True)
        parser.add_argument("--schema_json", required=True)
        parser.add_argument("--epochs", type=int, default=1)
        parser.add_argument("--batch_size", type=int, default=2)
        parser.add_argument("--learning_rate", type=float, default=2e-4)
        parser.add_argument("--grad_accum_steps", type=int, default=4)
        parser.add_argument("--precision", default="fp32")
        parser.add_argument("--quantization_bits", type=int, default=0)
        parser.add_argument("--lora_r", type=int, default=8)
        parser.add_argument("--lora_alpha", type=int, default=16)
        parser.add_argument("--lora_dropout", type=float, default=0.05)
        parser.add_argument("--target_modules", default="")
        parser.add_argument("--hf_token", default="")
        parser.add_argument("--lora_adapters", required=True)
        args = parser.parse_args()

        hf_token = get_token_value(args.hf_token)
        logger.info(f"HF Token present: {bool(hf_token)}")

        ensure_directory_exists(args.lora_adapters)
        output_dir = args.lora_adapters

        logger.info(f"Parsing schema from input")
        schema = json.loads(args.schema_json)
        logger.info(f"Schema loaded: {json.dumps(schema, indent=2)}")

        logger.info(f"Loading dataset from {args.processed_dataset}")
        dsd = load_from_disk(args.processed_dataset)

        logger.info(f"Loading model/tokenizer from: {args.model_name}")

        tokenizer_kwargs = {"use_fast": True, "trust_remote_code": True}
        if hf_token:
            tokenizer_kwargs["token"] = hf_token

        tokenizer = AutoTokenizer.from_pretrained(args.model_name, **tokenizer_kwargs)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            logger.info("Set pad_token to eos_token")

        model_kwargs = {"trust_remote_code": True, "torch_dtype": torch.float32}
        if hf_token:
            model_kwargs["token"] = hf_token

        if args.precision == "bf16":
            model_kwargs["torch_dtype"] = torch.bfloat16
        elif args.precision == "fp16":
            model_kwargs["torch_dtype"] = torch.float16

        try:
            model_kwargs["use_safetensors"] = True
            model = AutoModelForCausalLM.from_pretrained(args.model_name, **model_kwargs)
        except Exception as e:
            logger.warning(f"Failed to load with safetensors, trying without: {e}")
            model_kwargs.pop("use_safetensors", None)
            model = AutoModelForCausalLM.from_pretrained(args.model_name, **model_kwargs)

        logger.info(f"Model loaded successfully: {type(model).__name__}")

        if args.quantization_bits in (4, 8):
            logger.info(f"Preparing model for {args.quantization_bits}-bit training")
            model = prepare_model_for_kbit_training(model)

        if args.target_modules.strip():
            target_modules = [t.strip() for t in args.target_modules.split(",") if t.strip()]
            logger.info(f"Using user-specified LoRA targets: {target_modules}")
        else:
            target_modules = guess_lora_targets(model)

        lora_cfg = LoraConfig(
            r=args.lora_r,
            lora_alpha=args.lora_alpha,
            lora_dropout=args.lora_dropout,
            target_modules=target_modules,
            bias="none",
            task_type="CAUSAL_LM"
        )

        model = get_peft_model(model, lora_cfg)
        model.print_trainable_parameters()

        train_ds = dsd.get("train")
        has_messages = "messages" in train_ds.column_names if train_ds else False
        use_chat = has_messages and maybe_chat_template(tokenizer)
        logger.info(f"Dataset has messages format: {has_messages}")
        logger.info(f"Using chat template: {use_chat}")

        def tokenize_example(ex):
            if has_messages:
                if use_chat:
                    try:
                        msgs = ex.get("messages", [])
                        ids = tokenizer.apply_chat_template(
                            msgs,
                            tokenize=True,
                            add_generation_prompt=False,
                            truncation=True,
                            max_length=2048
                        )
                        return {"input_ids": ids, "attention_mask": [1] * len(ids)}
                    except Exception as e:
                        logger.warning("Chat template failed, using fallback: " + str(e))
                        msgs = ex.get("messages", [])
                        q = msgs[0].get("content", "") if len(msgs) > 0 else ""
                        a = msgs[1].get("content", "") if len(msgs) > 1 else ""
                        text = "User: " + q + "\\nAssistant: " + a
                else:
                    msgs = ex.get("messages", [])
                    q = msgs[0].get("content", "") if len(msgs) > 0 else ""
                    a = msgs[1].get("content", "") if len(msgs) > 1 else ""
                    text = "User: " + q + "\\nAssistant: " + a

                if not use_chat:
                    out = tokenizer(text, truncation=True, max_length=2048)
                    return {"input_ids": out["input_ids"], "attention_mask": out["attention_mask"]}
            else:
                input_field = schema.get("input_field", "text")
                text = ex.get(input_field, "")
                out = tokenizer(text, truncation=True, max_length=2048)
                return {"input_ids": out["input_ids"], "attention_mask": out["attention_mask"]}

        keep_cols = ["input_ids", "attention_mask"]
        logger.info("Tokenizing dataset...")
        train_cols = train_ds.column_names if train_ds else []
        cols_to_remove = [c for c in train_cols if c not in keep_cols]
        dsd = dsd.map(tokenize_example, remove_columns=cols_to_remove, desc="Tokenizing")

        collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
        train_dataset = dsd.get("train")
        train_len = len(train_dataset) if train_dataset else 0
        total_steps = max(1, math.ceil(train_len / (args.batch_size * args.grad_accum_steps)))

        training_args = TrainingArguments(
            output_dir=output_dir,
            per_device_train_batch_size=args.batch_size,
            gradient_accumulation_steps=args.grad_accum_steps,
            learning_rate=args.learning_rate,
            num_train_epochs=args.epochs,
            warmup_ratio=0.03,
            logging_steps=max(1, total_steps // 10),
            save_steps=total_steps,
            save_total_limit=1,
            report_to="none",
            bf16=(args.precision == "bf16"),
            fp16=(args.precision == "fp16"),
            save_safetensors=True,
            dataloader_num_workers=0,
            remove_unused_columns=False,
        )

        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            data_collator=collator
        )

        logger.info("=" * 60)
        logger.info("Starting LoRA fine-tuning...")
        logger.info(f"Training samples: {train_len}")
        logger.info(f"Total steps: {total_steps}")
        logger.info(f"Effective batch size: {args.batch_size * args.grad_accum_steps}")
        logger.info("=" * 60)

        trainer.train()

        adapter_dir = os.path.join(output_dir, "adapter")
        os.makedirs(adapter_dir, exist_ok=True)

        logger.info("Saving LoRA adapters to " + adapter_dir)
        model.save_pretrained(adapter_dir)
        tokenizer.save_pretrained(output_dir)

        run_config = {
            "model_name": args.model_name,
            "epochs": args.epochs,
            "batch_size": args.batch_size,
            "learning_rate": args.learning_rate,
            "grad_accum_steps": args.grad_accum_steps,
            "precision": args.precision,
            "quantization_bits": args.quantization_bits,
            "lora_r": args.lora_r,
            "lora_alpha": args.lora_alpha,
            "lora_dropout": args.lora_dropout,
            "target_modules": target_modules,
            "total_steps": total_steps,
            "num_train_samples": train_len
        }

        with open(os.path.join(output_dir, "run_config.json"), "w") as f:
            json.dump(run_config, f, indent=2)

        logger.info("=" * 60)
        logger.info("✅ Fine-tuning complete!")
        logger.info("→ LoRA adapters saved to: " + adapter_dir)
        logger.info("→ Tokenizer saved to: " + output_dir)
        logger.info("→ Configuration saved to: " + os.path.join(output_dir, "run_config.json"))
        logger.info("=" * 60)
        EOF
        
        # Execute the Python script with arguments from environment
        exec python3 /tmp/train_lora.py \
          --model_name "${MODEL_NAME}" \
          --processed_dataset "${PROCESSED_DATASET}" \
          --schema_json "${SCHEMA_JSON}" \
          --epochs "${EPOCHS}" \
          --batch_size "${BATCH_SIZE}" \
          --learning_rate "${LEARNING_RATE}" \
          --grad_accum_steps "${GRAD_ACCUM_STEPS}" \
          --precision "${PRECISION}" \
          --quantization_bits "${QUANTIZATION_BITS}" \
          --lora_r "${LORA_R}" \
          --lora_alpha "${LORA_ALPHA}" \
          --lora_dropout "${LORA_DROPOUT}" \
          --target_modules "${TARGET_MODULES}" \
          --hf_token "${HF_TOKEN}" \
          --lora_adapters "$1"
    args:
      - {outputPath: lora_adapters}
    env:
      - name: MODEL_NAME
        value: {inputValue: model_name}
      - name: PROCESSED_DATASET
        value: {inputPath: processed_dataset}
      - name: SCHEMA_JSON
        value: {inputValue: schema_json}
      - name: EPOCHS
        value: {inputValue: epochs}
      - name: BATCH_SIZE
        value: {inputValue: batch_size}
      - name: LEARNING_RATE
        value: {inputValue: learning_rate}
      - name: GRAD_ACCUM_STEPS
        value: {inputValue: grad_accum_steps}
      - name: PRECISION
        value: {inputValue: precision}
      - name: QUANTIZATION_BITS
        value: {inputValue: quantization_bits}
      - name: LORA_R
        value: {inputValue: lora_r}
      - name: LORA_ALPHA
        value: {inputValue: lora_alpha}
      - name: LORA_DROPOUT
        value: {inputValue: lora_dropout}
      - name: TARGET_MODULES
        value: {inputValue: target_modules}
      - name: HF_TOKEN
        value: {inputValue: hf_token}


