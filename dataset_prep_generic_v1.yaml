name: Generic Dataset Preparation
description: Prepares datasets for LoRA fine-tuning (SLM/LLM/VLM) from either Hugging Face repositories or PI API structured JSON. Automatically formats data into chat-style messages and saves schema metadata for downstream fine-tuning.

inputs:

  - {name: dataset_source, type: String, description: 'Either hf or pi'}
  - {name: dataset_uri, type: String, description: 'For HF dataset name or path. For PI API URL endpoint'}
  - {name: input_field, type: String, description: 'Column or key used for user prompt'}
  - {name: output_field, type: String, description: 'Column or key used for response'}  
  - {name: image_field, type: String, description: 'Optional column for image for multimodal data'}
  - {name: validation_split, type: Float, description: 'Validation split ratio'}
  - {name: max_records, type: Integer, description: 'Limit for quick local tests'}
  - {name: hf_token, type: String, description: 'Hugging Face access token for private datasets'}  
  - {name: access_token, type: String, description: 'Bearer access token for API auth'}     

outputs:
  - {name: processed_dataset, type: Dataset}
  - {name: schema_json, type: Data}   

implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import json
        import pandas as pd
        import requests
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        from datasets import load_dataset, Dataset, DatasetDict
        import logging
        import re
        
        # Ensure output directories exist before saving the files
        def ensure_directory_exists(file_path):
            directory = os.path.dirname(file_path)
            if not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)
        
        # Argument Parsing
        parser = argparse.ArgumentParser(description="Generic dataset preparation")
        parser.add_argument("--dataset_source", required=True)
        parser.add_argument("--dataset_uri", required=True)
        parser.add_argument("--input_field", required=True)
        parser.add_argument("--output_field", required=True)
        parser.add_argument("--image_field", default="")
        parser.add_argument("--validation_split", type=float, default=0.02)
        parser.add_argument("--max_records", type=int, default=2000)
        parser.add_argument("--hf_token", default=None)
        parser.add_argument("--access_token", default=None)
        parser.add_argument("--processed_dataset", required=True)
        parser.add_argument("--schema_json", required=True)
        args = parser.parse_args()
        
        def build_messages(example, input_col, output_col, image_col=None):
            user_msg = str(example.get(input_col, "")).strip()
            raw_output = example.get(output_col, "")
            if isinstance(raw_output, (dict, list)):
                assistant_msg = json.dumps(raw_output, ensure_ascii=False)
            else:
                assistant_msg = str(raw_output).strip()
            messages = []
            if image_col and image_col in example and example[image_col]:
                messages.append({"role": "user", "content": f"[IMAGE]: {example[image_col]}"})
            if user_msg:
                messages.append({"role": "user", "content": user_msg})
            if assistant_msg:
                messages.append({"role": "assistant", "content": assistant_msg})
            return messages
        
        def load_hf_dataset(uri, token=None):
            print(f"[INFO] Loading Hugging Face dataset: {uri}")
            if token:
                # Use token in the 'load_dataset' call if needed
                return load_dataset(uri, use_auth_token=token)
            return load_dataset(uri)
        
        def load_pi_api_dataset(api_url, access_token, input_field, output_field):
            print(f"[INFO] Fetching PI API dataset from {api_url}")
            session = requests.Session()
            retries = Retry(total=5, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
            session.mount("http://", HTTPAdapter(max_retries=retries))
            session.mount("https://", HTTPAdapter(max_retries=retries))
            headers = {"Authorization": f"Bearer {access_token}"} if access_token else {}
            resp = session.post(api_url, headers=headers, timeout=60)
            resp.raise_for_status()
            raw_data = resp.json()
        
            if isinstance(raw_data, list) and all(isinstance(x, dict) for x in raw_data) and input_field in raw_data[0]:
                print("[INFO] Flat PI API schema detected.")
                records = []
                for item in raw_data:
                    prompt = item.get(input_field, "")
                    response = item.get(output_field, "")
                    if isinstance(response, (dict, list)):
                        response = json.dumps(response, ensure_ascii=False)
                    if prompt or response:
                        records.append({"instruction": prompt, "output": response})
                return DatasetDict({"train": Dataset.from_list(records)})
        
            print("[INFO] Nested PI API schema detected")
            CHILD_KEYS = ["subsections", "sections", "children", "items"]
            TITLE_KEYS = ["title", "heading", "name"]
            CONTENT_KEYS = ["content", "text", "body"]
        
            def iter_children(node):
                for k in CHILD_KEYS:
                    v = node.get(k)
                    if isinstance(v, list):
                        return v
                return []
        
            def get_first(node, keys):
                for k in keys:
                    val = node.get(k)
                    if val:
                        return str(val).strip()
                return ""
        
            def extract_recursive(node, path=""):
                results = []
                title = get_first(node, TITLE_KEYS)
                content = get_first(node, CONTENT_KEYS)
                if title or content:
                    results.append({"title": title, "content": content, "path": path})
                for idx, child in enumerate(iter_children(node)):
                    subpath = f"{path}/sub_{idx}" if path else f"sub_{idx}"
                    results.extend(extract_recursive(child, subpath))
                return results
        
            def extract_all(raw):
                all_rows = []
                items = raw if isinstance(raw, list) else [raw]
                for resp in items:
                    resp_obj = resp.get("response", {})
                    for ir in resp_obj.get("individualResults", []):
                        orig = ir.get("originalResult", {})
                        for res in orig.get("results", []):
                            doc = res.get("data", {}).get("structuredTextJsonData", {}).get("document", {})
                            all_rows.extend(extract_recursive(doc))
                return all_rows
        
            flat_items = extract_all(raw_data)
            records = []
            for item in flat_items:
                prompt = item.get(input_field, "")
                response = item.get(output_field, "")
                if prompt or response:
                    records.append({"instruction": prompt, "output": response})
            return DatasetDict({"train": Dataset.from_list(records)})
        
        # Ensure output directories exist before saving the files
        ensure_directory_exists(args.processed_dataset)
        ensure_directory_exists(args.schema_json)
        
        # Load dataset
        if args.dataset_source.lower() == "huggingface":
            dataset = load_hf_dataset(args.dataset_uri, args.hf_token)
        elif args.dataset_source.lower() == "pi_api":
            if not args.access_token:
                raise ValueError("PI API mode requires access_token")
            dataset = load_pi_api_dataset(args.dataset_uri, args.access_token, args.input_field, args.output_field)
        else:
            raise ValueError(f"Unsupported dataset_source: {args.dataset_source}")
        
        train = dataset["train"]
        if args.max_records and len(train) > args.max_records:
            train = train.select(range(args.max_records))
        if "validation" in dataset:
            ds_dict = dataset
        else:
            split = train.train_test_split(test_size=args.validation_split, seed=42)
            ds_dict = DatasetDict({"train": split["train"], "validation": split["test"]})
        
        def add_messages(example):
            return {"messages": build_messages(example, args.input_field, args.output_field, args.image_field)}
        
        ds_dict = ds_dict.map(add_messages)
        ds_dict.save_to_disk(args.processed_dataset)
        
        # Create schema
        schema = {
            "dataset_source": args.dataset_source,
            "dataset_uri": args.dataset_uri,
            "input_field": args.input_field,
            "output_field": args.output_field,
            "image_field": args.image_field or None,
            "num_train": len(ds_dict["train"]),
            "num_val": len(ds_dict["validation"]),
            "source_type": args.dataset_source.lower()
        }
        
        # Save schema JSON
        with open(args.schema_json, "w") as f:
            json.dump(schema, f, indent=2)
        
        print("Dataset preparation complete")
        print(json.dumps(schema, indent=2))

    args:
      - --dataset_source
      - {inputValue: dataset_source}
      - --dataset_uri
      - {inputValue: dataset_uri}
      - --input_field
      - {inputValue: input_field}
      - --output_field
      - {inputValue: output_field}
      - --image_field
      - {inputValue: image_field}
      - --validation_split
      - {inputValue: validation_split}
      - --max_records
      - {inputValue: max_records}
      - --hf_token
      - {inputValue: hf_token}
      - --access_token
      - {inputValue: access_token}
      - --processed_dataset
      - {outputPath: processed_dataset}
      - --schema_json
      - {outputPath: schema_json}



