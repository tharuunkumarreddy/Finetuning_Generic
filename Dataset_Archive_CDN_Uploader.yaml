name: Dataset Archive CDN Uploader
description: Compresses the entire dataset directory structure (train, validation, schema.json, etc.) from Generic Dataset Preparation and uploads the archive to CDN. Returns the public CDN URL and decompression instructions.

inputs:
  - name: dataset_directory
    type: Data
    description: 'Output directory from Generic Dataset Preparation component containing train/, validation/, schema.json, etc.'
  - name: bearer_token
    type: string
    description: 'Bearer token for CDN authentication'
  - name: compression_format
    type: string
    description: 'Compression format: tar.gz, zip, or tar'
    default: 'tar.gz'

outputs:
  - name: archive_cdn_url
    type: string
    description: 'CDN URL where the compressed dataset archive is accessible'
  - name: archive_metadata
    type: Data
    description: 'JSON file containing archive info, decompression instructions, and original structure metadata'

implementation:
  container:
    image: python:3.9-slim
    command:
      - sh
      - -c
      - |
        apt-get update && apt-get install -y curl
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import subprocess
        import json
        import os
        import tarfile
        import zipfile
        import shutil
        from datetime import datetime

        parser = argparse.ArgumentParser(description="Compress dataset directory and upload to CDN.")
        parser.add_argument('--dataset_directory', type=str, required=True, 
                           help='Path to dataset directory from Generic Dataset Preparation.')
        parser.add_argument('--bearer_token', type=str, required=True, 
                           help='Bearer token for CDN authentication.')
        parser.add_argument('--compression_format', type=str, default='tar.gz',
                           choices=['tar.gz', 'tar', 'zip'],
                           help='Compression format to use.')
        parser.add_argument('--archive_cdn_url', type=str, required=True, 
                           help='Path to save the resulting CDN URL.')
        parser.add_argument('--archive_metadata', type=str, required=True, 
                           help='Path to save archive metadata JSON.')
        args = parser.parse_args()

        bearer_token = args.bearer_token
        upload_url = "https://igs.gov-cloud.ai/mobius-content-service/v1.0/content/upload?filePath=MPQE1&contentTags=dataset"

        def verify_dataset_structure(dataset_dir):
            """Verify the dataset directory has expected structure."""
            required_items = ['schema.json']
            optional_items = ['train', 'validation']
            
            if not os.path.isdir(dataset_dir):
                raise NotADirectoryError(f"Dataset directory not found: {dataset_dir}")
            
            found_items = os.listdir(dataset_dir)
            
            has_required = all(item in found_items for item in required_items)
            has_splits = any(item in found_items for item in optional_items)
            
            if not has_required:
                raise ValueError(f"Missing required files. Found: {found_items}")
            
            if not has_splits:
                raise ValueError(f"No train or validation splits found. Found: {found_items}")
            
            print(f"Dataset structure verified. Contents: {found_items}")
            return found_items

        def get_directory_size(path):
            """Calculate total directory size in bytes."""
            total = 0
            for dirpath, dirnames, filenames in os.walk(path):
                for filename in filenames:
                    filepath = os.path.join(dirpath, filename)
                    if os.path.exists(filepath):
                        total += os.path.getsize(filepath)
            return total

        def compress_directory(dataset_dir, output_archive, compression_format):
            """Compress directory in specified format."""
            dataset_name = os.path.basename(dataset_dir.rstrip('/'))
            
            print(f"Starting compression in {compression_format} format...")
            
            try:
                if compression_format == 'tar.gz':
                    with tarfile.open(output_archive, 'w:gz') as tar:
                        tar.add(dataset_dir, arcname=dataset_name)
                        
                elif compression_format == 'tar':
                    with tarfile.open(output_archive, 'w') as tar:
                        tar.add(dataset_dir, arcname=dataset_name)
                        
                elif compression_format == 'zip':
                    with zipfile.ZipFile(output_archive, 'w', zipfile.ZIP_DEFLATED) as zf:
                        for root, dirs, files in os.walk(dataset_dir):
                            for file in files:
                                file_path = os.path.join(root, file)
                                arcname = os.path.join(dataset_name, 
                                                      os.path.relpath(file_path, dataset_dir))
                                zf.write(file_path, arcname)
                
                archive_size = os.path.getsize(output_archive)
                print(f"Compression successful! Archive size: {archive_size:,} bytes ({archive_size/1024/1024:.2f} MB)")
                return archive_size
                
            except Exception as e:
                print(f"Compression failed: {e}")
                raise

        def upload_file_to_cdn(file_path, output_url_path):
            """Upload compressed archive to CDN."""
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"File not found: {file_path}")
            
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                raise ValueError("Archive file is empty")
            
            print(f"Uploading archive: {file_path}")
            print(f"File size: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)")
            
            curl_command = [
                "curl",
                "--location", upload_url,
                "--header", f"Authorization: Bearer {bearer_token}",
                "--form", f"file=@{file_path}",
                "--fail",
                "--show-error"
            ]
            
            print("Executing upload...")
            
            try:
                process = subprocess.run(
                    curl_command,
                    capture_output=True,
                    check=True
                )
                
                response_text = process.stdout.decode('utf-8')
                print("Upload successful!")
                
                response_json = json.loads(response_text)
                relative_cdn_url = response_json.get("cdnUrl")
                
                if not relative_cdn_url:
                    raise ValueError("CDN response missing 'cdnUrl' field")
                
                full_cdn_url = f"https://cdn-new.gov-cloud.ai{relative_cdn_url}"
                print(f"Archive available at: {full_cdn_url}")
                
                output_dir = os.path.dirname(output_url_path)
                if output_dir:
                    os.makedirs(output_dir, exist_ok=True)
                
                with open(output_url_path, "w") as f:
                    f.write(full_cdn_url)
                
                print(f"CDN URL saved to: {output_url_path}")
                return full_cdn_url
                
            except subprocess.CalledProcessError as e:
                print("Upload failed!")
                print(f"Exit code: {e.returncode}")
                print(f"Error output: {e.stderr.decode('utf-8')}")
                raise
            except json.JSONDecodeError as e:
                print("Failed to parse server response as JSON")
                print(f"Response was: {process.stdout.decode('utf-8')}")
                raise

        def save_metadata(metadata_path, metadata_dict):
            """Save metadata JSON file."""
            output_dir = os.path.dirname(metadata_path)
            if output_dir:
                os.makedirs(output_dir, exist_ok=True)
            
            with open(metadata_path, 'w') as f:
                json.dump(metadata_dict, f, indent=2)
            
            print(f"Metadata saved to: {metadata_path}")

        try:
            # Verify dataset structure
            print("Verifying dataset structure...")
            contents = verify_dataset_structure(args.dataset_directory)
            
            # Get original directory size
            original_size = get_directory_size(args.dataset_directory)
            print(f"Original dataset size: {original_size:,} bytes ({original_size/1024/1024:.2f} MB)")
            
            # Create archive
            archive_name = f"dataset_archive.{args.compression_format}"
            archive_path = os.path.join("/tmp", archive_name)
            
            archive_size = compress_directory(args.dataset_directory, archive_path, args.compression_format)
            
            # Upload to CDN
            cdn_url = upload_file_to_cdn(archive_path, args.archive_cdn_url)
            
            # Create metadata
            compression_ratio = (1 - archive_size / original_size) * 100 if original_size > 0 else 0
            
            metadata = {
                "upload_timestamp": datetime.utcnow().isoformat(),
                "cdn_url": cdn_url,
                "original_size_bytes": original_size,
                "archive_size_bytes": archive_size,
                "compression_ratio_percent": round(compression_ratio, 2),
                "compression_format": args.compression_format,
                "directory_contents": contents,
                "decompression_instructions": {
                    "tar.gz": f"tar -xzf {archive_name}",
                    "tar": f"tar -xf {archive_name}",
                    "zip": f"unzip {archive_name}"
                }[args.compression_format],
                "reloading_note": "After decompression, use load_from_disk() on the extracted directory to reload in Generic LoRA Fine-tuning component"
            }
            
            save_metadata(args.archive_metadata, metadata)
            
            print("\n" + "="*60)
            print("SUCCESS!")
            print(f"Archive available at: {cdn_url}")
            print(f"Compression ratio: {compression_ratio:.1f}%")
            print(f"Decompression command: {metadata['decompression_instructions']}")
            print("="*60)
            
        except Exception as e:
            print("\nUPLOAD FAILED!")
            print(f"Error: {e}")
            raise

    args:
      - --dataset_directory
      - {inputPath: dataset_directory}
      - --bearer_token
      - {inputValue: bearer_token}
      - --compression_format
      - {inputValue: compression_format}
      - --archive_cdn_url
      - {outputPath: archive_cdn_url}
      - --archive_metadata
      - {outputPath: archive_metadata}