name: Track LoRA Pipeline Essentials
description: Tracks essential metadata from LoRA fine-tuning pipeline - dataset, model, key hyperparameters, and results. Lightweight tracking focused on reproducibility.

inputs:
  - {name: dataset_schema_json, type: Data, description: 'Schema JSON from Generic Dataset Preparation'}
  - {name: lora_adapters_dir, type: Model, description: 'LoRA adapters directory containing run_config.json'}
  - {name: merged_model_dir, type: Model, description: 'Merged model directory (optional for tracking)'}
  - {name: execution_id, type: String, description: 'Unique execution ID for this pipeline run'}
  - {name: experiment_name, type: String, description: 'Human-readable experiment name', default: 'LoRA Experiment'}
  - {name: schema_id, type: String, description: 'The ID of the schema to update'}
  - {name: tenant_id, type: string, description: 'The ID of the tenant'}
  - {name: project_id, type: String, description: 'The ID of the project'}
  - {name: model_id, type: String, description: 'The ID of the model', default: '-1'}
  - {name: architecture_type, type: String, description: 'The architecture type', default: 'LoRA'}
  - {name: bearer_auth_token, type: string, description: 'Bearer token for authentication'}
  - {name: domain, type: String, description: 'The domain for the API endpoint'}

outputs:
  - {name: tracking_status, type: String, description: 'Status message from schema update'}

implementation:
  container:
    image: python:3.9-slim
    command:
      - sh
      - -c
      - |
        pip install requests > /dev/null 2>&1
        exec python3 -u -c "$0" "$@"
      - |
        import json
        import argparse
        import os
        import requests
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        from datetime import datetime

        parser = argparse.ArgumentParser()
        parser.add_argument('--dataset_schema_json', type=str, required=True)
        parser.add_argument('--lora_adapters_dir', type=str, required=True)
        parser.add_argument('--merged_model_dir', type=str, required=False, default='')
        parser.add_argument('--execution_id', type=str, required=True)
        parser.add_argument('--experiment_name', type=str, default='LoRA Experiment')
        parser.add_argument('--schema_id', type=str, required=True)
        parser.add_argument('--tenant_id', type=str, required=True)
        parser.add_argument('--project_id', type=str, required=True)
        parser.add_argument('--model_id', type=str, default='-1')
        parser.add_argument('--architecture_type', type=str, default='LoRA')
        parser.add_argument('--bearer_auth_token', type=str, required=True)
        parser.add_argument('--domain', type=str, required=True)
        parser.add_argument('--tracking_status', type=str, required=True)
        args = parser.parse_args()

        def ensure_directory_exists(file_path):
            directory = os.path.dirname(file_path)
            if directory and not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)

        def read_json_safe(file_path):
            if not os.path.exists(file_path):
                return {}
            try:
                with open(file_path, 'r') as f:
                    return json.load(f)
            except Exception as e:
                print("Warning - Could not read file - " + str(e))
                return {}

        print("=" * 80)
        print("TRACKING LORA PIPELINE ESSENTIALS")
        print("=" * 80)

        print("")
        print("Step 1 of 3 - Reading dataset metadata")
        dataset_info = read_json_safe(args.dataset_schema_json)
        
        dataset_source = dataset_info.get('dataset_source', 'unknown')
        dataset_uri = dataset_info.get('dataset_uri', 'unknown')
        num_train = dataset_info.get('num_train', 0)
        num_val = dataset_info.get('num_val', 0)
        
        print("Dataset source = " + str(dataset_source))
        print("Dataset URI = " + str(dataset_uri))
        print("Train samples = " + str(num_train))
        print("Val samples = " + str(num_val))

        print("")
        print("Step 2 of 3 - Reading training configuration")
        run_config_path = os.path.join(args.lora_adapters_dir, "run_config.json")
        training_config = read_json_safe(run_config_path)
        
        base_model = training_config.get('model_name', 'unknown')
        epochs = training_config.get('epochs', 0)
        batch_size = training_config.get('batch_size', 0)
        learning_rate = training_config.get('learning_rate', 0.0)
        lora_r = training_config.get('lora_r', 0)
        lora_alpha = training_config.get('lora_alpha', 0)
        
        print("Base model = " + str(base_model))
        print("Epochs = " + str(epochs))
        print("Batch size = " + str(batch_size))
        print("Learning rate = " + str(learning_rate))
        print("LoRA rank = " + str(lora_r))
        print("LoRA alpha = " + str(lora_alpha))

        print("")
        print("Step 3 of 3 - Checking merged model")
        merged_model_exists = False
        merged_model_path = 'not_merged'
        
        if args.merged_model_dir and os.path.exists(args.merged_model_dir):
            merged_model_exists = True
            merged_model_path = args.merged_model_dir
            print("Merged model found at " + str(merged_model_path))
        else:
            print("Merged model not provided")

        print("")
        print("Step 4 of 4 - Preparing tracking data")
        
        tracking_data = {
            "experiment_name": args.experiment_name,
            "execution_timestamp": datetime.utcnow().isoformat() + "Z",
            "dataset_source": dataset_source,
            "dataset_name": dataset_uri,
            "num_training_samples": str(num_train) + " train, " + str(num_val) + " val",
            "base_model_name": base_model,
            "adapter_location": args.lora_adapters_dir,
            "merged_model_location": merged_model_path,
            "architecture_type": args.architecture_type
        }

        print("Tracking " + str(len(tracking_data)) + " essential fields")

        print("")
        print("=" * 80)
        print("UPDATING SCHEMA DATABASE")
        print("=" * 80)

        with open(args.bearer_auth_token, 'r') as f:
            bearer_auth_token = f.read().strip()

        with open(args.tenant_id, 'r') as f:
            tenant_id = f.read().strip()

        mapping = {
            "experiment_id": "experiment_name",
            "timestamp": "execution_timestamp",
            "datasets_type": "dataset_source",
            "datasets_url": "dataset_name",
            "datasets_keys": "num_training_samples",
            "model_name": "base_model_name",
            "model_type": "architecture_type",
            "adapter_url": "adapter_location",
            "merged_model_url": "merged_model_location",
            "model_url": "base_model_name"
        }

        print("")
        print("Execution ID = " + str(args.execution_id))
        print("Schema ID = " + str(args.schema_id))

        headers = {
            'Content-Type': 'application/json',
            'Authorization': 'Bearer ' + bearer_auth_token
        }

        retry_strategy = Retry(
            total=3,
            status_forcelist=[408, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "PUT", "POST", "DELETE", "OPTIONS", "TRACE"],
            backoff_factor=1
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        http = requests.Session()
        http.mount("https://", adapter)
        http.mount("http://", adapter)

        check_url = args.domain + "/pi-entity-instances-service/v3.0/schemas/" + args.schema_id + "/instances/list"
        
        # Try to convert execution_id to integer if it's numeric, otherwise keep as string
        execution_id_value = args.execution_id
        try:
            execution_id_value = int(args.execution_id)
            print("Using integer execution_id = " + str(execution_id_value))
        except ValueError:
            print("Using string execution_id = " + str(execution_id_value))
        
        check_payload = {
            "dbType": "TIDB",
            "ownedOnly": True,
            "filter": {
                "execution_id": execution_id_value
            }
        }
        
        print("")
        print("Checking for existing record")

        try:
            response = http.post(check_url, headers=headers, data=json.dumps(check_payload), timeout=60)
            response.raise_for_status()
            response_data = response.json()
            
            if response_data.get("content"):
                print("Found existing record - UPDATING")
                
                patch_requests = []
                for schema_column, data_key in mapping.items():
                    if data_key in tracking_data:
                        value = tracking_data[data_key]
                        
                        if isinstance(value, bool):
                            value_to_patch = value
                        elif isinstance(value, (int, float)):
                            value_to_patch = value
                        else:
                            value_to_patch = str(value)

                        patch_requests.append({
                            "operation": "REPLACE",
                            "path": schema_column,
                            "value": value_to_patch
                        })

                update_url = args.domain + "/pi-entity-instances-service/v2.0/schemas/" + args.schema_id + "/instances"
                update_payload = {
                    "dbType": "TIDB",
                    "conditionalFilter": {
                        "conditions": [
                            {
                                "field": "execution_id",
                                "operator": "EQUAL",
                                "value": execution_id_value
                            }
                        ]
                    },
                    "partialUpdateRequests": [
                        {
                            "patch": patch_requests
                        }
                    ]
                }
                
                print("Updating " + str(len(patch_requests)) + " fields")
                
                response = http.patch(update_url, headers=headers, data=json.dumps(update_payload), timeout=60)
                response.raise_for_status()
                
                print("Successfully updated existing record")
                status_message = "Updated existing record for execution_id = " + str(args.execution_id)
                
            else:
                print("No existing record - CREATING NEW")
                
                creation_data = {}
                for schema_column, data_key in mapping.items():
                    if data_key in tracking_data:
                        value = tracking_data[data_key]
                        
                        if isinstance(value, bool):
                            value_to_add = value
                        elif isinstance(value, (int, float)):
                            value_to_add = value
                        else:
                            value_to_add = str(value)
                        
                        creation_data[schema_column] = value_to_add
                
                creation_data['execution_id'] = execution_id_value
                if args.model_id != '-1':
                    creation_data['id'] = args.model_id
                if args.project_id != '-1':
                    creation_data['project_id'] = args.project_id
                if args.architecture_type != '-1':
                    creation_data['model_type'] = args.architecture_type

                create_url = args.domain + "/pi-entity-instances-service/v2.0/schemas/" + args.schema_id + "/instances"
                create_payload = {
                    "data": [creation_data]
                }

                print("Creating record with " + str(len(creation_data)) + " fields")
                
                response = http.post(create_url, headers=headers, data=json.dumps(create_payload), timeout=60)
                if response.status_code >= 400:
                    print("Error " + str(response.status_code) + " " + str(response.reason))
                    print("Response " + str(response.text))
                response.raise_for_status()
                
                print("Successfully created new record")
                status_message = "Created new record for execution_id = " + str(args.execution_id)
            
            print("API Response " + str(response.json()))

        except requests.exceptions.RequestException as e:
            error_msg = "Schema update failed - " + str(e)
            print("")
            print(error_msg)
            if hasattr(e, 'response') and e.response is not None:
                print("Status " + str(e.response.status_code))
                print("Response " + str(e.response.text))
            status_message = error_msg

        ensure_directory_exists(args.tracking_status)
        with open(args.tracking_status, 'w') as f:
            f.write(status_message)

        print("")
        print("=" * 80)
        print("TRACKING COMPLETE")
        print("=" * 80)
        print("Status = " + str(status_message))
        print("=" * 80)
      - --dataset_schema_json
      - {inputPath: dataset_schema_json}
      - --lora_adapters_dir
      - {inputPath: lora_adapters_dir}
      - --merged_model_dir
      - {inputPath: merged_model_dir}
      - --execution_id
      - {inputValue: execution_id}
      - --experiment_name
      - {inputValue: experiment_name}
      - --schema_id
      - {inputValue: schema_id}
      - --tenant_id
      - {inputPath: tenant_id}
      - --project_id
      - {inputValue: project_id}
      - --model_id
      - {inputValue: model_id}
      - --architecture_type
      - {inputValue: architecture_type}
      - --bearer_auth_token
      - {inputPath: bearer_auth_token}
      - --domain
      - {inputValue: domain}
      - --tracking_status
      - {outputPath: tracking_status}
