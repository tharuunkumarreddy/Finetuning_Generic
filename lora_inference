name: Universal LoRA Inference
description: Inferences any fine-tuned model using LoRA adapters.

inputs:
  - {name: base_model, type: String, description: 'Base model name (must match training)'}
  - {name: adapter_path, type: String, description: 'Path to LoRA adapters'}
  - {name: prompt, type: String, description: 'Single prompt for inference'}
  - {name: device, type: String, description: 'Device to run inference on: auto, cuda, or cpu'}
  - {name: max_tokens, type: Integer, description: 'Maximum tokens to generate'}
  - {name: temperature, type: Float, description: 'Sampling temperature'}

outputs:
  - {name: response, type: String, description: 'Generated response for the prompt'}

metadata:
  annotations:
    author: Universal LoRA Inference Component

implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
      - (pip install --no-cache-dir peft > /dev/null 2>&1) && "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import torch
        from transformers import AutoTokenizer, AutoModelForCausalLM
        from peft import PeftModel
        import argparse

        class UniversalLoRAInference:
            """Universal inference engine that adapts to any fine-tuned model"""

            def __init__(self, base_model_name, adapter_path, device="auto"):
                self.device = device
                self.base_model_name = base_model_name
                self.adapter_path = adapter_path

                print("=" * 70)
                print("Universal LoRA Inference Engine")
                print("=" * 70)

                # Auto-detect configuration
                self.config = self._load_training_config()
                self.schema = self._load_schema()

                # Load tokenizer
                print(f"Loading tokenizer: {base_model_name}")
                self.tokenizer = AutoTokenizer.from_pretrained(
                    base_model_name,
                    trust_remote_code=True
                )

                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token

                # Load base model
                print(f"Loading base model: {base_model_name}")
                base_model = AutoModelForCausalLM.from_pretrained(
                    base_model_name,
                    torch_dtype=torch.float32,
                    device_map=device,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )

                # Auto-detect adapter directory
                adapter_dir = self._find_adapter_directory()
                print(f"Loading LoRA adapters: {adapter_dir}")

                self.model = PeftModel.from_pretrained(base_model, adapter_dir)
                self.model.eval()

                # Display info
                self._display_info()
                print("=" * 70)
                print()

            def _find_adapter_directory(self):
                """Automatically find the adapter directory"""
                # Try common locations
                possible_paths = [
                    self.adapter_path,
                    os.path.join(self.adapter_path, "adapter"),
                    os.path.join(self.adapter_path, "data", "adapter"),
                    os.path.join(self.adapter_path, "data")
                ]

                for path in possible_paths:
                    adapter_config = os.path.join(path, "adapter_config.json")
                    if os.path.exists(adapter_config):
                        return path

                # If not found, use the provided path
                return self.adapter_path

            def _load_training_config(self):
                """Load training configuration if available"""
                config_paths = [
                    os.path.join(self.adapter_path, "run_config.json"),
                    os.path.join(self.adapter_path, "data", "run_config.json")
                ]

                for path in config_paths:
                    if os.path.exists(path):
                        with open(path, 'r') as f:
                            return json.load(f)
                return {}

            def _load_schema(self):
                """Load dataset schema if available"""
                schema_paths = [
                    os.path.join(self.adapter_path, "schema.json"),
                    os.path.join(self.adapter_path, "data", "schema.json")
                ]

                for path in schema_paths:
                    if os.path.exists(path):
                        with open(path, 'r') as f:
                            return json.load(f)
                return {}

            def _display_info(self):
                """Display model and training information"""
                print("\n" + "-" * 70)
                print("MODEL INFORMATION")
                print("-" * 70)
                print(f"Base Model: {self.base_model_name}")
                print(f"Device: {self.model.device}")

                if self.config:
                    print(f"\nTraining Config:")
                    print(f"  - Epochs: {self.config.get('epochs', 'N/A')}")
                    print(f"  - Learning Rate: {self.config.get('learning_rate', 'N/A')}")
                    print(f"  - LoRA Rank (r): {self.config.get('lora_r', 'N/A')}")
                    print(f"  - Training Samples: {self.config.get('num_train_samples', 'N/A')}")

                if self.schema:
                    print(f"\nDataset Info:")
                    print(f"  - Source: {self.schema.get('dataset_uri', 'N/A')}")
                    print(f"  - Input Field: {self.schema.get('input_field', 'N/A')}")
                    print(f"  - Output Field: {self.schema.get('output_field', 'N/A')}")

            def generate(
                self,
                prompt,
                max_new_tokens=150,
                temperature=0.7,
                top_p=0.9,
                top_k=50,
                do_sample=True,
                repetition_penalty=1.1
            ):
                """
                Generate response for any prompt

                Args:
                    prompt: Input text
                    max_new_tokens: Max tokens to generate
                    temperature: Sampling temperature
                    top_p: Nucleus sampling
                    top_k: Top-k sampling
                    do_sample: Use sampling vs greedy
                    repetition_penalty: Penalty for repetition

                Returns:
                    Generated text
                """
                # Format prompt (universal format that works for most cases)
                formatted_prompt = f"User: {prompt}\nAssistant:"

                # Tokenize
                inputs = self.tokenizer(
                    formatted_prompt,
                    return_tensors="pt",
                    truncation=True,
                    max_length=512
                )

                # Move to device
                inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

                # Generate
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=max_new_tokens,
                        temperature=temperature,
                        top_p=top_p,
                        top_k=top_k,
                        do_sample=do_sample,
                        repetition_penalty=repetition_penalty,
                        pad_token_id=self.tokenizer.pad_token_id,
                        eos_token_id=self.tokenizer.eos_token_id
                    )

                # Decode
                full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

                # Extract only the generated part (after "Assistant:")
                if "Assistant:" in full_response:
                    response = full_response.split("Assistant:")[-1].strip()
                else:
                    # Fallback: remove the input prompt
                    response = full_response[len(formatted_prompt):].strip()

                return response

        def main():
            # Initialize engine
            engine = UniversalLoRAInference(
                base_model_name="{{inputs.base_model}}",
                adapter_path="{{inputs.adapter_path}}",
                device="{{inputs.device}}"
            )

            # Single prompt mode
            response = engine.generate(
                "{{inputs.prompt}}",
                max_new_tokens={{inputs.max_tokens}},
                temperature={{inputs.temperature}}
            )
            # Output the response
            {{outputs.response}} = response

        if __name__ == "__main__":
            main()

    args:
      - --base_model
      - {inputValue: base_model}
      - --adapter_path
      - {inputValue: adapter_path}
      - --prompt
      - {inputValue: prompt}
      - --device
      - {inputValue: device}
      - --max_tokens
      - {inputValue: max_tokens}
      - --temperature
      - {inputValue: temperature}
