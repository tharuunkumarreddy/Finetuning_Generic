name: Generic LoRA Inference
description: Universal inference component for any LoRA fine-tuned model. Supports single prompt, batch prompts, and interactive mode.

inputs:
  - {name: base_model, type: String, description: 'Base model name (must match training model)'}
  - {name: lora_adapters, type: Model, description: 'LoRA adapters from Generic LoRA Fine-tuning component'}
  - {name: prompt, type: String, description: 'Input prompt for inference. Use empty string for batch mode'}
  - {name: batch_prompts, type: String, description: 'JSON array of prompts for batch inference. Example: ["prompt1", "prompt2"]'}
  - {name: max_tokens, type: Integer, description: 'Maximum tokens to generate'}
  - {name: temperature, type: Float, description: 'Sampling temperature (0.0 to 2.0)'}
  - {name: top_p, type: Float, description: 'Nucleus sampling parameter'}
  - {name: top_k, type: Integer, description: 'Top-k sampling parameter'}
  - {name: repetition_penalty, type: Float, description: 'Penalty for repetition'}
  - {name: hf_token, type: String, description: 'Hugging Face access token for gated models'}

outputs:
  - {name: inference_results, type: Data}

metadata:
  annotations:
    author: Generic LoRA Inference Component

implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
      - (pip install --no-cache-dir peft > /dev/null 2>&1) && "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import json
        import torch
        from transformers import AutoTokenizer, AutoModelForCausalLM
        from peft import PeftModel
        import logging
        
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger('lora_inference')
        
        def ensure_directory_exists(file_path):
            directory = os.path.dirname(file_path)
            if directory and not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)
        
        def get_token_value(token_arg):
            if not token_arg:
                return None
            token_str = str(token_arg).strip()
            if os.path.exists(token_str):
                try:
                    with open(token_str, 'r') as f:
                        content = f.read().strip()
                        return content if content else None
                except Exception as e:
                    logger.error(f"Failed to read token file: {e}")
                    return None
            else:
                return token_str if token_str and token_str != "None" else None
        
        class UniversalLoRAInference:
            def __init__(self, base_model_name, adapter_path, hf_token=None, device="auto"):
                self.device = device
                self.base_model_name = base_model_name
                self.adapter_path = adapter_path
                
                logger.info("="*70)
                logger.info("Universal LoRA Inference Engine")
                logger.info("="*70)
                
                self.config = self._load_training_config()
                self.schema = self._load_schema()
                
                logger.info(f"Loading tokenizer: {base_model_name}")
                tokenizer_kwargs = {"trust_remote_code": True}
                if hf_token:
                    tokenizer_kwargs["token"] = hf_token
                
                self.tokenizer = AutoTokenizer.from_pretrained(
                    base_model_name,
                    **tokenizer_kwargs
                )
                
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                
                logger.info(f"Loading base model: {base_model_name}")
                model_kwargs = {
                    "torch_dtype": torch.float32,
                    "device_map": device,
                    "trust_remote_code": True,
                    "low_cpu_mem_usage": True
                }
                if hf_token:
                    model_kwargs["token"] = hf_token
                
                base_model = AutoModelForCausalLM.from_pretrained(
                    base_model_name,
                    **model_kwargs
                )
                
                adapter_dir = self._find_adapter_directory()
                logger.info(f"Loading LoRA adapters: {adapter_dir}")
                
                self.model = PeftModel.from_pretrained(base_model, adapter_dir)
                self.model.eval()
                
                self._display_info()
                logger.info("="*70)
            
            def _find_adapter_directory(self):
                possible_paths = [
                    self.adapter_path,
                    os.path.join(self.adapter_path, "adapter"),
                    os.path.join(self.adapter_path, "data", "adapter"),
                    os.path.join(self.adapter_path, "data")
                ]
                
                for path in possible_paths:
                    adapter_config = os.path.join(path, "adapter_config.json")
                    if os.path.exists(adapter_config):
                        return path
                
                return self.adapter_path
            
            def _load_training_config(self):
                config_paths = [
                    os.path.join(self.adapter_path, "run_config.json"),
                    os.path.join(self.adapter_path, "data", "run_config.json")
                ]
                
                for path in config_paths:
                    if os.path.exists(path):
                        with open(path, 'r') as f:
                            return json.load(f)
                return {}
            
            def _load_schema(self):
                schema_paths = [
                    os.path.join(self.adapter_path, "schema.json"),
                    os.path.join(self.adapter_path, "data", "schema.json")
                ]
                
                for path in schema_paths:
                    if os.path.exists(path):
                        with open(path, 'r') as f:
                            return json.load(f)
                return {}
            
            def _display_info(self):
                logger.info("-"*70)
                logger.info("MODEL INFORMATION")
                logger.info("-"*70)
                logger.info(f"Base Model: {self.base_model_name}")
                logger.info(f"Device: {self.model.device}")
                
                if self.config:
                    logger.info("Training Config:")
                    logger.info(f"  - Epochs: {self.config.get('epochs', 'N/A')}")
                    logger.info(f"  - Learning Rate: {self.config.get('learning_rate', 'N/A')}")
                    logger.info(f"  - LoRA Rank (r): {self.config.get('lora_r', 'N/A')}")
                    logger.info(f"  - Training Samples: {self.config.get('num_train_samples', 'N/A')}")
                
                if self.schema:
                    logger.info("Dataset Info:")
                    logger.info(f"  - Source: {self.schema.get('dataset_uri', 'N/A')}")
                    logger.info(f"  - Input Field: {self.schema.get('input_field', 'N/A')}")
                    logger.info(f"  - Output Field: {self.schema.get('output_field', 'N/A')}")
            
            def generate(self, prompt, max_new_tokens=150, temperature=0.7, top_p=0.9, top_k=50, do_sample=True, repetition_penalty=1.1):
                user_prefix = "User: "
                assistant_prefix = "Assistant:"
                newline = "\n"
                formatted_prompt = user_prefix + prompt + newline + assistant_prefix
                
                inputs = self.tokenizer(formatted_prompt, return_tensors="pt", truncation=True, max_length=512)
                
                input_ids = inputs["input_ids"].to(self.model.device)
                attention_mask = inputs["attention_mask"].to(self.model.device)
                
                with torch.no_grad():
                    outputs = self.model.generate(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        max_new_tokens=max_new_tokens,
                        temperature=temperature,
                        top_p=top_p,
                        top_k=top_k,
                        do_sample=do_sample,
                        repetition_penalty=repetition_penalty,
                        pad_token_id=self.tokenizer.pad_token_id,
                        eos_token_id=self.tokenizer.eos_token_id
                    )
                
                full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                if assistant_prefix in full_response:
                    parts = full_response.split(assistant_prefix)
                    response = parts[-1].strip()
                else:
                    prompt_len = len(formatted_prompt)
                    response = full_response[prompt_len:].strip()
                
                return response
            
            def batch_generate(self, prompts, **kwargs):
                responses = []
                for i, prompt in enumerate(prompts):
                    logger.info(f"Processing prompt {i+1}/{len(prompts)}")
                    response = self.generate(prompt, **kwargs)
                    responses.append(response)
                return responses
        
        parser = argparse.ArgumentParser(description="Universal LoRA Inference")
        parser.add_argument("--base_model", required=True)
        parser.add_argument("--lora_adapters", required=True)
        parser.add_argument("--prompt", default="")
        parser.add_argument("--batch_prompts", default="")
        parser.add_argument("--max_tokens", type=int, default=150)
        parser.add_argument("--temperature", type=float, default=0.7)
        parser.add_argument("--top_p", type=float, default=0.9)
        parser.add_argument("--top_k", type=int, default=50)
        parser.add_argument("--repetition_penalty", type=float, default=1.1)
        parser.add_argument("--hf_token", default=None)
        parser.add_argument("--inference_results", required=True)
        args = parser.parse_args()
        
        hf_token = get_token_value(args.hf_token)
        logger.info(f"HF Token present: {bool(hf_token)}")
        
        if hf_token:
            from huggingface_hub import login
            try:
                login(token=hf_token)
                logger.info("Logged in to Hugging Face Hub")
            except Exception as e:
                logger.error(f"Failed to login to HF: {e}")
        
        ensure_directory_exists(args.inference_results)
        
        if not os.path.exists(args.lora_adapters):
            raise ValueError(f"LoRA adapters path not found: {args.lora_adapters}")
        
        engine = UniversalLoRAInference(
            base_model_name=args.base_model,
            adapter_path=args.lora_adapters,
            hf_token=hf_token,
            device="auto"
        )
        
        results = {
            "base_model": args.base_model,
            "adapter_path": args.lora_adapters,
            "generation_config": {
                "max_tokens": args.max_tokens,
                "temperature": args.temperature,
                "top_p": args.top_p,
                "top_k": args.top_k,
                "repetition_penalty": args.repetition_penalty
            },
            "inferences": []
        }
        
        if args.batch_prompts and args.batch_prompts.strip():
            try:
                prompts = json.loads(args.batch_prompts)
                if not isinstance(prompts, list):
                    prompts = [prompts]
                logger.info(f"Batch inference mode with {len(prompts)} prompts")
            except json.JSONDecodeError:
                prompts = [args.batch_prompts]
                logger.info("Single prompt from batch_prompts parameter")
            
            responses = engine.batch_generate(
                prompts,
                max_new_tokens=args.max_tokens,
                temperature=args.temperature,
                top_p=args.top_p,
                top_k=args.top_k,
                repetition_penalty=args.repetition_penalty
            )
            
            for prompt, response in zip(prompts, responses):
                results["inferences"].append({
                    "prompt": prompt,
                    "response": response
                })
                logger.info(f"Prompt: {prompt}")
                logger.info(f"Response: {response}")
                logger.info("-"*70)
        
        elif args.prompt and args.prompt.strip():
            logger.info("Single inference mode")
            logger.info(f"Prompt: {args.prompt}")
            
            response = engine.generate(
                args.prompt,
                max_new_tokens=args.max_tokens,
                temperature=args.temperature,
                top_p=args.top_p,
                top_k=args.top_k,
                repetition_penalty=args.repetition_penalty
            )
            
            results["inferences"].append({
                "prompt": args.prompt,
                "response": response
            })
            
            logger.info(f"Response: {response}")
            logger.info("-"*70)
        
        else:
            logger.info("Demo mode with sample prompts")
            demo_prompts = [
                "What is machine learning?",
                "Explain artificial intelligence.",
                "How does deep learning work?"
            ]
            
            responses = engine.batch_generate(
                demo_prompts,
                max_new_tokens=args.max_tokens,
                temperature=args.temperature,
                top_p=args.top_p,
                top_k=args.top_k,
                repetition_penalty=args.repetition_penalty
            )
            
            for prompt, response in zip(demo_prompts, responses):
                results["inferences"].append({
                    "prompt": prompt,
                    "response": response
                })
                logger.info(f"Prompt: {prompt}")
                logger.info(f"Response: {response}")
                logger.info("-"*70)
        
        with open(args.inference_results, "w") as f:
            json.dump(results, f, indent=2)
        
        logger.info("="*70)
        logger.info("Inference complete!")
        logger.info(f"Results saved to: {args.inference_results}")
        logger.info(f"Total inferences: {len(results['inferences'])}")
        logger.info("="*70)
    args:
      - --base_model
      - {inputValue: base_model}
      - --lora_adapters
      - {inputPath: lora_adapters}
      - --prompt
      - {inputValue: prompt}
      - --batch_prompts
      - {inputValue: batch_prompts}
      - --max_tokens
      - {inputValue: max_tokens}
      - --temperature
      - {inputValue: temperature}
      - --top_p
      - {inputValue: top_p}
      - --top_k
      - {inputValue: top_k}
      - --repetition_penalty
      - {inputValue: repetition_penalty}
      - --hf_token
      - {inputValue: hf_token}
      - --inference_results
      - {outputPath: inference_results}
