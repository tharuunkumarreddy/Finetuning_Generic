name: Generic LoRA Fine-tuning Unsloth119
description: Fine-tunes any LLM using PEFT LoRA adapters with Unsloth optimization for 2-5x speed and memory efficiency.

inputs:
  - {name: model_name, type: String, description: 'Hugging Face model ID or path'}
  - {name: processed_dataset, type: Data, description: 'Prepared dataset from Generic Dataset Preparation component'}
  - {name: schema_json, type: Data, description: 'Schema metadata JSON from Generic Dataset Preparation component'}
  - {name: epochs, type: Integer, description: 'Number of training epochs'}
  - {name: batch_size, type: Integer, description: 'Per-device training batch size'}
  - {name: learning_rate, type: Float, description: 'Learning rate for optimizer'}
  - {name: grad_accum_steps, type: Integer, description: 'Gradient accumulation steps'}
  - {name: precision, type: String, description: 'Training precision: fp32, fp16, or bf16'}
  - {name: quantization_bits, type: Integer, description: 'Quantization bits (0, 4, or 8)'}
  - {name: lora_r, type: Integer, description: 'LoRA rank parameter'}
  - {name: lora_alpha, type: Integer, description: 'LoRA alpha parameter'}
  - {name: lora_dropout, type: Float, description: 'LoRA dropout rate'}
  - {name: target_modules, type: String, description: 'Comma-separated LoRA target modules'}
  - {name: max_seq_length, type: Integer, description: 'Maximum sequence length'}
  - {name: use_unsloth, type: String, description: 'Enable Unsloth optimization: true or false (default: true)'}
  - {name: hf_token, type: String, description: 'Hugging Face access token for gated models'}

outputs:
  - {name: lora_adapters, type: Model}

metadata:
  annotations:
    author: Generic LoRA Fine-tuning with Unsloth

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v24-gpu
    command:
      - sh
      - -c
      - |
        pip install --no-cache-dir torch==2.5.1 torchvision==0.20.1 --index-url https://download.pytorch.org/whl/cu121 && \
        pip install --no-cache-dir transformers==4.46.3 peft==0.13.2 trl==0.11.4 bitsandbytes==0.44.1 accelerate==0.34.2 datasets && \
        "$0" "$@"
      - python3
      - -u
      - -c
      - |
        def _make_parent_dirs_and_return_path(file_path):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path
        
        def train_lora(
            model_name_path,
            processed_dataset_path,
            schema_json_path,
            epochs_val,
            batch_size_val,
            learning_rate_val,
            grad_accum_steps_val,
            precision_val,
            quantization_bits_val,
            lora_r_val,
            lora_alpha_val,
            lora_dropout_val,
            target_modules_val,
            max_seq_length_val,
            use_unsloth_val,
            hf_token_val,
            lora_adapters_path,
        ):
            import os
            import json
            import math
            import logging
            
            logging.basicConfig(level=logging.INFO)
            logger = logging.getLogger("lora_training")
            
            # Check GPU availability first
            import torch
            has_gpu = torch.cuda.is_available()
            logger.info(f"GPU Available: {has_gpu}")
            if has_gpu:
                logger.info(f"GPU Device: {torch.cuda.get_device_name(0)}")
                logger.info(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
            
            # Try importing Unsloth only if GPU is available and enabled
            unsloth_available = False
            FastLanguageModel = None
            
            if use_unsloth_val and has_gpu:
                try:
                    from unsloth import FastLanguageModel
                    unsloth_available = True
                    logger.info("✅ Unsloth library available - optimization enabled")
                except ImportError as e:
                    logger.warning(f"⚠️ Unsloth import failed: {e}")
                    logger.info("Falling back to standard training")
                except NotImplementedError as e:
                    logger.warning(f"⚠️ Unsloth initialization failed: {e}")
                    logger.info("Falling back to standard training")
                except Exception as e:
                    logger.warning(f"⚠️ Unexpected Unsloth error: {e}")
                    logger.info("Falling back to standard training")
            elif use_unsloth_val and not has_gpu:
                logger.warning("⚠️ GPU not detected - Unsloth requires GPU acceleration")
                logger.info("Falling back to standard training")
            else:
                logger.info("Unsloth optimization disabled by user configuration")
            
            # Import transformers libraries after checking Unsloth
            from datasets import load_from_disk
            from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments
            from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
            from trl import SFTTrainer
            
            def ensure_directory_exists(file_path):
                directory = os.path.dirname(file_path)
                if directory and not os.path.exists(directory):
                    os.makedirs(directory, exist_ok=True)
            
            def get_token_value(token_arg):
                if not token_arg:
                    return None
                token_str = str(token_arg).strip()
                if os.path.exists(token_str):
                    try:
                        with open(token_str) as f:
                            content = f.read().strip()
                            return content if content else None
                    except Exception as e:
                        logger.error(f"Failed to read token file: {e}")
                        return None
                else:
                    return token_str if token_str and token_str != "None" else None
            
            def maybe_chat_template(tokenizer):
                tmpl = getattr(tokenizer, "chat_template", None)
                return tmpl is not None and len(str(tmpl)) > 0
            
            def guess_lora_targets(model):
                names = [n for n, _ in model.named_modules()]
                logger.info("Analyzing model architecture for LoRA targets...")
                if any("q_proj" in n for n in names):
                    targets = ["q_proj", "k_proj", "v_proj", "o_proj"]
                elif any("c_attn" in n for n in names):
                    targets = ["c_attn", "c_proj"]
                elif any("fc1" in n for n in names):
                    targets = ["fc1", "fc2"]
                elif any("Wqkv" in n for n in names):
                    targets = ["Wqkv"]
                elif any("query_key_value" in n for n in names):
                    targets = ["query_key_value"]
                else:
                    logger.warning("Could not auto-detect LoRA targets; defaulting to common linear layers")
                    targets = ["q_proj", "k_proj", "v_proj", "o_proj"]
                logger.info(f"Selected LoRA target modules: {targets}")
                return targets
            
            hf_token = get_token_value(hf_token_val)
            logger.info(f"HF Token present: {bool(hf_token)}")
            
            from huggingface_hub import login
            if hf_token:
                try:
                    login(token=hf_token)
                    logger.info("Logged in to Hugging Face Hub")
                except Exception as e:
                    logger.error(f"Failed to login to HF: {e}")
            
            ensure_directory_exists(lora_adapters_path)
            output_dir = lora_adapters_path
            
            logger.info(f"Loading schema from {schema_json_path}")
            with open(schema_json_path) as f:
                schema = json.load(f)
            logger.info(f"Schema loaded: {json.dumps(schema, indent=2)}")
            
            logger.info(f"Loading dataset from {processed_dataset_path}")
            dsd = load_from_disk(processed_dataset_path)
            
            # Determine dtype for model loading
            dtype_map = {
                "bf16": torch.bfloat16,
                "fp16": torch.float16,
                "fp32": torch.float32
            }
            model_dtype = dtype_map.get(precision_val, None)
            
            # Determine if 4-bit quantization should be used
            load_in_4bit = quantization_bits_val == 4
            load_in_8bit = quantization_bits_val == 8
            
            logger.info(f"Loading model/tokenizer from: {model_name_path}")
            logger.info(f"Precision: {precision_val}, Quantization: {quantization_bits_val}-bit")
            logger.info(f"Max sequence length: {max_seq_length_val}")
            logger.info(f"Use Unsloth: {use_unsloth_val and unsloth_available}")
            
            model = None
            tokenizer = None
            
            # Try loading with Unsloth first if enabled and available
            if use_unsloth_val and unsloth_available and FastLanguageModel is not None:
                try:
                    logger.info("=" * 60)
                    logger.info("Loading model with Unsloth optimization...")
                    logger.info("=" * 60)
                    
                    model, tokenizer = FastLanguageModel.from_pretrained(
                        model_name=model_name_path,
                        max_seq_length=max_seq_length_val,
                        dtype=model_dtype,
                        load_in_4bit=load_in_4bit,
                        token=hf_token
                    )
                    
                    logger.info("✅ Model loaded successfully with Unsloth")
                    
                    # Parse target modules
                    cleaned_target_modules = target_modules_val.strip().strip('"').strip("'")
                    if cleaned_target_modules and cleaned_target_modules != "None":
                        target_modules = [t.strip() for t in cleaned_target_modules.split(",") if t.strip()]
                        if not target_modules:
                            target_modules = guess_lora_targets(model)
                    else:
                        target_modules = guess_lora_targets(model)
                    
                    logger.info(f"Applying Unsloth LoRA with targets: {target_modules}")
                    
                    # Apply Unsloth's optimized LoRA
                    model = FastLanguageModel.get_peft_model(
                        model,
                        r=lora_r_val,
                        target_modules=target_modules,
                        lora_alpha=lora_alpha_val,
                        lora_dropout=lora_dropout_val,
                        bias="none",
                        use_gradient_checkpointing="unsloth",
                        random_state=3407,
                        use_rslora=False,
                        loftq_config=None
                    )
                    
                    logger.info("✅ Unsloth LoRA adapters applied successfully")
                    
                except Exception as e:
                    logger.warning(f"⚠️ Unsloth loading failed: {e}")
                    logger.info("Falling back to standard transformers...")
                    model = None
                    tokenizer = None
            
            # Fallback to standard transformers if Unsloth failed or disabled
            if model is None:
                logger.info("=" * 60)
                logger.info("Loading model with standard transformers...")
                logger.info("=" * 60)
                
                tokenizer_kwargs = {"use_fast": True, "trust_remote_code": True}
                if hf_token:
                    tokenizer_kwargs["token"] = hf_token
                
                tokenizer = AutoTokenizer.from_pretrained(model_name_path, **tokenizer_kwargs)
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                    logger.info("Set pad_token to eos_token")
                
                model_kwargs = {
                    "trust_remote_code": True,
                    "device_map": "auto"
                }
                
                if model_dtype:
                    model_kwargs["torch_dtype"] = model_dtype
                
                if load_in_4bit:
                    from transformers import BitsAndBytesConfig
                    model_kwargs["quantization_config"] = BitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_compute_dtype=model_dtype or torch.float16,
                        bnb_4bit_use_double_quant=True,
                        bnb_4bit_quant_type="nf4"
                    )
                elif load_in_8bit:
                    model_kwargs["load_in_8bit"] = True
                
                if hf_token:
                    model_kwargs["token"] = hf_token
                
                try:
                    logger.info(f"Attempting to load model with kwargs: {list(model_kwargs.keys())}")
                    model = AutoModelForCausalLM.from_pretrained(model_name_path, **model_kwargs)
                except Exception as e:
                    logger.error(f"Failed to load model: {e}")
                    raise
                
                logger.info(f"✅ Model loaded: {type(model).__name__}")
                
                if quantization_bits_val in (4, 8):
                    logger.info(f"Preparing model for {quantization_bits_val}-bit training")
                    model = prepare_model_for_kbit_training(model)
                
                # Parse target modules
                cleaned_target_modules = target_modules_val.strip().strip('"').strip("'")
                if cleaned_target_modules and cleaned_target_modules != "None":
                    target_modules = [t.strip() for t in cleaned_target_modules.split(",") if t.strip()]
                    if not target_modules:
                        target_modules = guess_lora_targets(model)
                else:
                    target_modules = guess_lora_targets(model)
                
                lora_cfg = LoraConfig(
                    r=lora_r_val,
                    lora_alpha=lora_alpha_val,
                    lora_dropout=lora_dropout_val,
                    target_modules=target_modules,
                    bias="none",
                    task_type="CAUSAL_LM"
                )
                
                model = get_peft_model(model, lora_cfg)
                logger.info("✅ Standard LoRA adapters applied")
            
            # Ensure tokenizer pad token is set
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
                logger.info("Set pad_token to eos_token")
            
            model.print_trainable_parameters()
            
            train_ds = dsd.get("train")
            has_messages = "messages" in train_ds.column_names if train_ds else False
            use_chat = has_messages and maybe_chat_template(tokenizer)
            logger.info(f"Dataset has messages format: {has_messages}")
            logger.info(f"Using chat template: {use_chat}")
            
            def tokenize_example(ex):
                max_len = min(max_seq_length_val, getattr(tokenizer, 'model_max_length', max_seq_length_val))
                if max_len > max_seq_length_val:
                    max_len = max_seq_length_val
                
                if has_messages:
                    if use_chat:
                        try:
                            msgs = ex.get("messages", [])
                            ids = tokenizer.apply_chat_template(msgs, tokenize=True, add_generation_prompt=False, truncation=True, max_length=max_len)
                            return {"input_ids": ids, "attention_mask": [1] * len(ids)}
                        except Exception as e:
                            logger.warning("Chat template failed, using fallback: " + str(e))
                            msgs = ex.get("messages", [])
                            q = msgs[0].get("content", "") if len(msgs) > 0 else ""
                            a = msgs[1].get("content", "") if len(msgs) > 1 else ""
                            text = "User: " + q + " Assistant: " + a
                    else:
                        msgs = ex.get("messages", [])
                        q = msgs[0].get("content", "") if len(msgs) > 0 else ""
                        a = msgs[1].get("content", "") if len(msgs) > 1 else ""
                        text = "User: " + q + " Assistant: " + a
                    if not use_chat:
                        out = tokenizer(text, truncation=True, max_length=max_len)
                        return {"input_ids": out["input_ids"], "attention_mask": out["attention_mask"]}
                else:
                    input_field = schema.get("input_field", "text")
                    text = ex.get(input_field, "")
                    out = tokenizer(text, truncation=True, max_length=max_len)
                    return {"input_ids": out["input_ids"], "attention_mask": out["attention_mask"]}
            
            keep_cols = ["input_ids", "attention_mask"]
            logger.info("Tokenizing dataset...")
            train_cols = train_ds.column_names if train_ds else []
            cols_to_remove = [c for c in train_cols if c not in keep_cols]
            dsd = dsd.map(tokenize_example, remove_columns=cols_to_remove, desc="Tokenizing")
            
            train_dataset = dsd.get("train")
            train_len = len(train_dataset) if train_dataset else 0
            total_steps = max(1, math.ceil(train_len / (batch_size_val * grad_accum_steps_val)) * epochs_val)
            
            # Configure training arguments based on whether using Unsloth or not
            training_args_dict = {
                "output_dir": output_dir,
                "per_device_train_batch_size": batch_size_val,
                "gradient_accumulation_steps": grad_accum_steps_val,
                "learning_rate": learning_rate_val,
                "num_train_epochs": epochs_val,
                "warmup_ratio": 0.03,
                "logging_steps": max(1, total_steps // 10),
                "save_steps": total_steps,
                "save_total_limit": 1,
                "report_to": "none",
                "bf16": (precision_val == "bf16"),
                "fp16": (precision_val == "fp16"),
                "save_safetensors": True,
                "dataloader_num_workers": 0,
                "remove_unused_columns": False,
            }
            
            # Add Unsloth-specific optimizations if using Unsloth
            if use_unsloth_val and unsloth_available and FastLanguageModel is not None:
                training_args_dict.update({
                    "optim": "adamw_8bit",
                    "weight_decay": 0.01,
                    "lr_scheduler_type": "linear",
                    "seed": 3407,
                })
                logger.info("✅ Using Unsloth-optimized training configuration")
            
            training_args = TrainingArguments(**training_args_dict)
            
            # Enable Unsloth training mode if applicable
            if use_unsloth_val and unsloth_available and FastLanguageModel is not None:
                try:
                    FastLanguageModel.for_training(model)
                    logger.info("✅ Unsloth training mode enabled")
                except Exception as e:
                    logger.warning(f"Could not enable Unsloth training mode: {e}")
            
            # Use standard Trainer with DataCollator
            collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=train_dataset,
                data_collator=collator
            )
            
            logger.info("=" * 60)
            logger.info("Starting LoRA fine-tuning...")
            logger.info(f"Training samples: {train_len}")
            logger.info(f"Total steps: {total_steps}")
            logger.info(f"Effective batch size: {batch_size_val * grad_accum_steps_val}")
            logger.info(f"Optimization mode: {'Unsloth (2-5x faster)' if (use_unsloth_val and unsloth_available) else 'Standard'}")
            logger.info("=" * 60)
            
            trainer.train()
            
            adapter_dir = os.path.join(output_dir, "adapter")
            os.makedirs(adapter_dir, exist_ok=True)
            
            logger.info("Saving LoRA adapters to " + adapter_dir)
            model.save_pretrained(adapter_dir)
            tokenizer.save_pretrained(output_dir)
            
            run_config = {
                "model_name": model_name_path,
                "epochs": epochs_val,
                "batch_size": batch_size_val,
                "learning_rate": learning_rate_val,
                "grad_accum_steps": grad_accum_steps_val,
                "precision": precision_val,
                "quantization_bits": quantization_bits_val,
                "lora_r": lora_r_val,
                "lora_alpha": lora_alpha_val,
                "lora_dropout": lora_dropout_val,
                "target_modules": target_modules if 'target_modules' in locals() else [],
                "max_seq_length": max_seq_length_val,
                "used_unsloth": use_unsloth_val and unsloth_available,
                "total_steps": total_steps,
                "num_train_samples": train_len
            }
            
            with open(os.path.join(output_dir, "run_config.json"), "w") as f:
                json.dump(run_config, f, indent=2)
            
            logger.info("=" * 60)
            logger.info("Fine-tuning complete!")
            logger.info("LoRA adapters saved to: " + adapter_dir)
            logger.info("Tokenizer saved to: " + output_dir)
            logger.info("Configuration saved to: " + os.path.join(output_dir, "run_config.json"))
            logger.info("=" * 60)
        
        import argparse
        _parser = argparse.ArgumentParser(prog="Generic LoRA Fine-tuning with Unsloth", description="Fine-tunes any LLM using PEFT LoRA adapters with Unsloth optimization")
        _parser.add_argument("--model_name", dest="model_name_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--processed_dataset", dest="processed_dataset_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--schema_json", dest="schema_json_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--epochs", dest="epochs_val", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--batch_size", dest="batch_size_val", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--learning_rate", dest="learning_rate_val", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--grad_accum_steps", dest="grad_accum_steps_val", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--precision", dest="precision_val", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--quantization_bits", dest="quantization_bits_val", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--lora_r", dest="lora_r_val", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--lora_alpha", dest="lora_alpha_val", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--lora_dropout", dest="lora_dropout_val", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--target_modules", dest="target_modules_val", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--max_seq_length", dest="max_seq_length_val", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--use_unsloth", dest="use_unsloth_val", type=lambda x: x.lower() == 'true', required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--hf_token", dest="hf_token_val", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--lora_adapters", dest="lora_adapters_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())
        train_lora(**_parsed_args)

    args:
      - --model_name
      - {inputValue: model_name}
      - --processed_dataset
      - {inputPath: processed_dataset}
      - --schema_json
      - {inputPath: schema_json}
      - --epochs
      - {inputValue: epochs}
      - --batch_size
      - {inputValue: batch_size}
      - --learning_rate
      - {inputValue: learning_rate}
      - --grad_accum_steps
      - {inputValue: grad_accum_steps}
      - --precision
      - {inputValue: precision}
      - --quantization_bits
      - {inputValue: quantization_bits}
      - --lora_r
      - {inputValue: lora_r}
      - --lora_alpha
      - {inputValue: lora_alpha}
      - --lora_dropout
      - {inputValue: lora_dropout}
      - --target_modules
      - {inputValue: target_modules}
      - --max_seq_length
      - {inputValue: max_seq_length}
      - --use_unsloth
      - {inputValue: use_unsloth}
      - --hf_token
      - {inputValue: hf_token}
      - --lora_adapters
      - {outputPath: lora_adapters}
